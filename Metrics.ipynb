{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "840c1346-ef90-450e-9299-f45dd614e46e",
   "metadata": {},
   "source": [
    "# Calculate Metrics Segmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2722849a-51f1-4d71-b687-23aa27693666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please update your data path to the correct folder (should contain train, val and test folders).\n"
     ]
    }
   ],
   "source": [
    "# # Make Nifti1Image from the segmentation results\n",
    "data_path = \"/home/jovyan/Project/ACDC/database\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9610ee88-bdcb-48e4-a90d-6484897ae6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import monai\n",
    "from PIL import Image\n",
    "import torch\n",
    "import wandb\n",
    "import nibabel as nib\n",
    "from skimage import metrics\n",
    "from sklearn.metrics import jaccard_score\n",
    "import scipy.ndimage as ndi\n",
    "from monai.config import print_config\n",
    "from monai.utils import first\n",
    "from monai.config import KeysCollection\n",
    "from monai.data import Dataset, ArrayDataset, create_test_image_3d, DataLoader\n",
    "from monai.transforms import (\n",
    "    Transform,\n",
    "    MapTransform,\n",
    "    Randomizable,\n",
    "    AddChannel,\n",
    "    AddChanneld,\n",
    "    CastToTyped,\n",
    "    Compose,\n",
    "    EnsureChannelFirst,\n",
    "    LoadImage,\n",
    "    LoadImaged,\n",
    "    Lambda,\n",
    "    Lambdad,\n",
    "    RandSpatialCrop,\n",
    "    RandSpatialCropd,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    "    ToTensord,\n",
    "    Orientation, \n",
    "    Rotate,\n",
    "    RandFlipd,\n",
    "    RandAffined,\n",
    "    RandGaussianNoised,\n",
    "    RandRotated\n",
    ")\n",
    "import random\n",
    "import math\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb50b469-10d4-4289-a31d-41d126c4d626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build function that writes list of paths to images automatically, based on the root folder of the data and the filename structure:\n",
    "def build_dict(data_path, mode):\n",
    "    dicts = [] \n",
    "    patient_folders = glob.glob(os.path.join(data_path, mode, 'patient*'))\n",
    "    for patient_folder in patient_folders:\n",
    "        patient_id = os.path.basename(patient_folder)\n",
    "        all_paths = glob.glob(os.path.join(patient_folder, f'{patient_id}_frame*.nii.gz')) # path to all 'frame' image files (both mask and img)\n",
    "        time_path = os.path.join(patient_folder,f'{patient_id}_4d.nii.gz') # path to the '4d' image, so the image acquired over time\n",
    "        frame_indices = sorted(list(set([os.path.basename(path).split('_frame')[-1].split('.')[0] for path in all_paths])))\n",
    "        for i in range(0,len(frame_indices),2): # loop over every other frame index since you only want the number (and also got '_gt' in frame_indices)\n",
    "            frame_index = frame_indices[i]\n",
    "            if frame_index == '01':\n",
    "                img_ED = f'{patient_id}_frame{frame_index}.nii.gz' # diastole images are always frame 01\n",
    "                mask_ED = f'{patient_id}_frame{frame_index}_gt.nii.gz'\n",
    "                ED_img_path = os.path.join(patient_folder,img_ED)\n",
    "                ED_mask_path = os.path.join(patient_folder,mask_ED)\n",
    "            else:\n",
    "                img_ES = f'{patient_id}_frame{frame_index}.nii.gz' # systole images are the other frame (number varies)\n",
    "                mask_ES = f'{patient_id}_frame{frame_index}_gt.nii.gz'\n",
    "                ES_img_path = os.path.join(patient_folder,img_ES)\n",
    "                ES_mask_path = os.path.join(patient_folder,mask_ES)\n",
    "        dicts.append({'ED_img': ED_img_path, 'ED_mask': ED_mask_path, 'ES_img': ES_img_path, 'ES_mask': ES_mask_path})#,'time_img':time_path})  \n",
    "    return dicts\n",
    "\n",
    "train_dict = build_dict(data_path, 'training')\n",
    "val_dict = build_dict(data_path, 'testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd189bcc-48d8-4a76-a601-5c1e17603ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a monai transform that loads the data from the dataset (so it retrieves the image via the path)\n",
    "class LoadData(monai.transforms.Transform):\n",
    "    def __init__(self, keys=None):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        ES_image = nib.load(sample['ES_img']).get_fdata().astype(np.float64) # load Nifti image and transform to numpy\n",
    "        ED_image = nib.load(sample['ED_img']).get_fdata().astype(np.float64) \n",
    "        ES_mask = nib.load(sample['ES_mask']).get_fdata().astype(np.float64)\n",
    "        ED_mask = nib.load(sample['ED_mask']).get_fdata().astype(np.float64)\n",
    "\n",
    "        # The function then returns the images and corresponding binary masks containing all 4 labels\n",
    "        return {'ES_img': ES_image, \n",
    "                'ES_mask': ES_mask,\n",
    "                'ED_img': ED_image, \n",
    "                'ED_mask': ED_mask}\n",
    "\n",
    "train_dataset = monai.data.Dataset(train_dict, transform = LoadData())\n",
    "val_dataset = monai.data.Dataset(val_dict, transform = LoadData())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3b1c234-e335-4283-938a-de4da2eefdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function with which we can easily keep track of the transforms: necessary for wandb to recognise them (compose to list and the otherway around)\n",
    "def from_compose_to_list(transform_compose):\n",
    "    \"\"\"\n",
    "    Transform an object monai.transforms.Compose in a list fully describing the transform.\n",
    "    /!\\ Random seed is not saved, then reproducibility is not enabled.\n",
    "    \"\"\"\n",
    "    from copy import deepcopy\n",
    "        \n",
    "    if not isinstance(transform_compose, monai.transforms.Compose):\n",
    "        raise TypeError(\"transform_compose should be a monai.transforms.Compose object.\")\n",
    "    \n",
    "    output_list = list()\n",
    "    for transform in transform_compose.transforms:\n",
    "        kwargs = deepcopy(vars(transform))\n",
    "        \n",
    "        # Remove attributes which are not arguments\n",
    "        args = list(transform.__init__.__code__.co_varnames[1: transform.__init__.__code__.co_argcount])\n",
    "        for key, obj in vars(transform).items():\n",
    "            if key not in args:\n",
    "                del kwargs[key]\n",
    "\n",
    "        output_list.append({\"class\": transform.__class__, \"kwargs\": kwargs})\n",
    "    return output_list\n",
    "\n",
    "def from_list_to_compose(transform_list):\n",
    "    \"\"\"\n",
    "    Transform a list in the corresponding monai.transforms.Compose object.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not isinstance(transform_list, list):\n",
    "        raise TypeError(\"transform_list should be a list.\")\n",
    "    \n",
    "    pre_compose_list = list()\n",
    "    \n",
    "    for transform_dict in transform_list:\n",
    "        if not isinstance(transform_dict, dict) or 'class' not in transform_dict or 'kwargs' not in transform_dict:\n",
    "            raise TypeError(\"transform_list should only contains dicts with keys ['class', 'kwargs']\")\n",
    "        \n",
    "        try:\n",
    "            transform = transform_dict['class'](**transform_dict['kwargs'])\n",
    "        except TypeError: # Classes have been converted to str after saving\n",
    "            transform = eval(transform_dict['class'].replace(\"__main__.\", \"\"))(**transform_dict['kwargs'])\n",
    "            \n",
    "        pre_compose_list.append(transform)\n",
    "        \n",
    "    return monai.transforms.Compose(pre_compose_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ce228ff-e833-4205-8756-e46a86e6076e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<class 'monai.transforms.utility.array.AddChannel'>: Class `AddChannel` has been deprecated since version 0.8. please use MetaTensor data type and monai.transforms.EnsureChannelFirst instead.\n",
      "Loading dataset: 100%|██████████| 50/50 [00:04<00:00, 10.37it/s]\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation and making datasets for training and validation:\n",
    "\n",
    "keys_img = ['ES_img', 'ED_img']\n",
    "keys_mask = ['ES_mask','ED_mask']\n",
    "maxlen = 512 # maximum dimension of the original samples, so resize to that dimension\n",
    "max_size = [512, 512, 16]\n",
    "roi_size = [336,336,16]\n",
    "\n",
    "# Only the necessary transforms to be able to load the data into the UNet. This will be applied to all keys present: \n",
    "T_val = monai.transforms.Compose([\n",
    "    LoadData(),\n",
    "    monai.transforms.AddChanneld(keys=val_dict[0].keys()),\n",
    "    monai.transforms.ScaleIntensityd(keys = keys_img, minv = 0, maxv = 1),\n",
    "    monai.transforms.Resized(keys=val_dict[0].keys(),size_mode='all',spatial_size=max_size, mode= 'nearest'), # resize to the largest dimension present in the original images    \n",
    "    monai.transforms.CenterSpatialCropd(keys=val_dict[0].keys(),roi_size=roi_size), # crop the images so that the UNet trains more quickly\n",
    "])\n",
    "\n",
    "val_dataset_T = monai.data.CacheDataset(val_dict, transform=T_val) # transforms necessary for the UNet\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec1e376a-4993-493c-b17a-77aa42940a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "val_dataloader = DataLoader(val_dataset_T, batch_size=batch_size,shuffle=True) # create batches of 16 images (shuffled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba851ccf-0c47-419f-9d2d-cc2b22759a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The used device is cuda:5\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:5\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device =\"cpu\"\n",
    "print(f'The used device is {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db6f49aa-61eb-4cbd-b004-a53f5eb30dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = monai.networks.nets.UNet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=3,\n",
    "    channels=(8, 16, 32, 64, 128),\n",
    "    strides=(2, 2, 2, 2),\n",
    "    num_res_units=2,\n",
    ").to(device)\n",
    "\n",
    "# Load model the new different way\n",
    "lr = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_function =  monai.losses.DiceCELoss(sigmoid=True, batch=True)\n",
    "\n",
    "\n",
    "# MODEL PATH\n",
    "checkpoint = torch.load('/home/jovyan/Project/Val/ACDCtrainedUNet_ED_lr_0.001_3_13_41_56_60 (1).pt',map_location = torch.device(device))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f6aab0-557d-47d7-b70e-ef80413f6607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a few results (only the segmentation and og image, not the gt mask because we didn't load that):\n",
    "\n",
    "def visualize_results(state, slice, model, dataloader):\n",
    "    model.eval()  # set the model to evaluation mode\n",
    "    with torch.no_grad():  # turn off gradient computation bc we're not training anymore\n",
    "        no_batches = len(val_dict)/batch_size # get the total number of batches: no_samples/batch_size\n",
    "        no_batches = math.trunc(no_batches) # round down so you're certain you don't get a batch that doesn't exist\n",
    "        batch = random.randrange(start=1, stop=no_batches) # generate random batch number within total number of batches\n",
    "        \n",
    "        # Get images and labels from the dataset, and push the images through the model to get an output image\n",
    "        for batch in dataloader:\n",
    "            images = batch[f'{state}_img'].float().to(device) \n",
    "            output = model(images)\n",
    "\n",
    "            i = random.randrange(images.shape[0]) # generate random sample number for in batch\n",
    "            print(i) # will now give 0 because batchsize = 1\n",
    "            j = 0 # mask number, 0 1 or 2\n",
    "            labelname = ['RV','MYO','LV']\n",
    "            # Visualize the images, ground truth masks, and model output masks\n",
    "            img = images[i,:,:,:].cpu().numpy()\n",
    "            img = np.squeeze(img)\n",
    "            pred_mask = output[i,j,:,:,:].cpu().numpy()\n",
    "            # Squeeze the mask where you specified j for\n",
    "            pred_mask = np.squeeze(pred_mask)\n",
    "            \n",
    "            # Plot only z-axis for now\n",
    "            img_plane = img[:,:,slice]\n",
    "            pred_mask_plane = pred_mask[:,:,slice]\n",
    "\n",
    "            # Display the images and masks \n",
    "            fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "            axes[0].imshow(img_plane, cmap='gray')\n",
    "            axes[0].set_title('OG image')\n",
    "            \n",
    "            axes[1].imshow(pred_mask_plane)\n",
    "            axes[1].set_title(f'Pred mask with label {labelname[j]}')     \n",
    "            \n",
    "            # Visualisatie mbv sigmoids:\n",
    "            sigmoid = torch.nn.Sigmoid()\n",
    "            pred_mask0 = 1*torch.round(sigmoid(output[i,0,:,:,:])).cpu().numpy()\n",
    "            pred_mask1 = 2*torch.round(sigmoid(output[i,1,:,:,:])).cpu().numpy()\n",
    "            pred_mask2 = 3*torch.round(sigmoid(output[i,2,:,:,:])).cpu().numpy()\n",
    "            overlay_mask0 = np.ma.masked_where(pred_mask0 == 0, pred_mask0 == 1) # this masks elements in the mask that are 0, and leaves elements that are 1 unmasked\n",
    "            overlay_mask1 = np.ma.masked_where(pred_mask1 == 0, pred_mask1 == 1)\n",
    "            overlay_mask2 = np.ma.masked_where(pred_mask2 == 0, pred_mask2 == 1)\n",
    "            axes[2].imshow(img_plane, cmap='gray')\n",
    "            axes[2].imshow(overlay_mask0[:,:,slice], 'summer', alpha=0.7, clim=[0, 1], interpolation='nearest')\n",
    "            axes[2].imshow(overlay_mask1[:,:,slice], 'autumn', alpha=0.7, clim=[0, 1], interpolation='nearest')\n",
    "            axes[2].imshow(overlay_mask2[:,:,slice], 'winter', alpha=0.7, clim=[0, 1], interpolation='nearest')\n",
    "            axes[2].set_title('Total mask')\n",
    "            plt.show() \n",
    "\n",
    "slice = 3 # slice in z-axis used for visualization\n",
    "state = 'ED'\n",
    "visualize_results(state,slice,model,val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b52b7cc-338f-45d2-aaa5-f436b667794c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_mask(labels, gt):\n",
    "    masks = []\n",
    "\n",
    "    for batch in range(len(labels)):\n",
    "        if gt == True:\n",
    "            label = np.array(labels[batch])[0] # make an numpy array from the labels, where we remove the first channel (= 1, was needed for transforms) and take a batch. Original dimensions labels = [1, no_masks, x, y, z]\n",
    "        else:\n",
    "            label = np.array(labels[batch])  \n",
    "        # Split masks into seperate masks based on label number\n",
    "        mask1 = label == 1 # RV\n",
    "        mask2 = label == 2 # MYO\n",
    "        mask3 = label == 3 # LV\n",
    "        \n",
    "        # Merge all three masks together, containing values 1, 2, and 3:\n",
    "        real_ES_mask = [] # preallocate\n",
    "\n",
    "        real_ES_mask.append(1*mask1)\n",
    "        real_ES_mask.append(2*mask2)\n",
    "        real_ES_mask.append(3*mask3)\n",
    "\n",
    "        masks.append(real_ES_mask)\n",
    "        # print(masks.shape)\n",
    "    masks=monai.data.MetaTensor(np.array(masks))\n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "712dca13-58e7-4ff3-ba90-7901265f97a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Make a classification report of the segmentation\n",
    "def calculate_report(gt, seg):\n",
    "    gt = gt[0].flatten()\n",
    "    seg = seg[0].flatten()\n",
    "    \n",
    "    target_names = ['BG', 'RV', 'MYO', 'LV']\n",
    "    return classification_report(gt, seg,  target_names=target_names, output_dict = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c40e0ead-b236-4fcc-9fe9-c25e99bf3317",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import directed_hausdorff\n",
    "\n",
    "# Calculate the Huausdorff distance\n",
    "def calculate_DH(gt, seg, slice):\n",
    "    return directed_hausdorff(gt[:,:,slice], seg[:,:,slice])[0]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1696aa3-8645-4237-83ee-8b3f08ce10d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add all the masks\n",
    "def post_processing(output):\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    output_softmax = torch.nn.functional.softmax(output, dim=1) # probability # dimension of the mask layers is 1 (counting from 0)\n",
    "    output_argmax = torch.argmax(output_softmax, dim=1) # max van probability, output is indices over die as\n",
    "    output_argmax = (output_argmax + 1).astype(np.int64)\n",
    "    \n",
    "    binary_masks = torch.round(sigmoid(output[0])).cpu().detach().numpy() # make binary masks (still different layers)\n",
    "    binary_mask = np.sum([binary_masks[0,:,:,:],binary_masks[1,:,:,:], binary_masks[2,:,:,:]],axis = 0)\n",
    "    binary_mask = torch.from_numpy(binary_mask)\n",
    "    binary_mask = torch.round(sigmoid(binary_mask)).cpu().numpy()\n",
    "    binary_mask = np.expand_dims(binary_mask, axis=0)\n",
    "    \n",
    "    argbin = output_argmax*binary_mask\n",
    "    return argbin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8bf657f6-27af-4fab-ba8a-e934b2dd52fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # set the model to evaluation mode\n",
    "\n",
    "rep_array = [[], [], []]\n",
    "HD_array = []\n",
    "\n",
    "for batch in val_dataloader:\n",
    "    images = batch[f'ED_img'].float().to(device)\n",
    "    labels = batch[f'ED_mask'].float()\n",
    "    output = model(images)\n",
    "\n",
    "    img = images[0,:,:,:].cpu().numpy()\n",
    "\n",
    "    pred_mask = output[0,0,:,:,:].detach().cpu().numpy()\n",
    "    # Squeeze the mask where you specified j for\n",
    "    pred_mask = np.squeeze(pred_mask)\n",
    "\n",
    "    img_plane = img[:,:,slice]\n",
    "    pred_mask_plane = pred_mask[:,:,slice]\n",
    "\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    pp_output = post_processing(output)\n",
    "\n",
    "    rep = calculate_report(labels[0], pp_output)\n",
    "    rep_array[0].append(rep[\"RV\"][\"f1-score\"])\n",
    "    rep_array[1].append(rep[\"MYO\"][\"f1-score\"])\n",
    "    rep_array[2].append(rep[\"LV\"][\"f1-score\"])\n",
    "    HD_array.append(calculate_DH(labels[0][0], pp_output[0], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c37d12a5-dda4-4981-b61c-60e075ad56f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hausdorff min:  0.0\n",
      "Hausdorff max:  12.84523257866513\n",
      "Hausdorff avg:  6.793926837319098\n"
     ]
    }
   ],
   "source": [
    "print(\"Hausdorff min: \",min(HD_array))\n",
    "print(\"Hausdorff max: \",max(HD_array))\n",
    "print(\"Hausdorff avg: \",np.average(HD_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "da4e9ef2-e49d-46d5-9dee-531c21473a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RV Dice min:  0.5289883890487442\n",
      "RV Dice max:  0.9385712801689808\n",
      "RV Dice avg:  0.8230100600337217\n"
     ]
    }
   ],
   "source": [
    "print(\"RV Dice min: \",min(rep_array[0]))\n",
    "print(\"RV Dice max: \",max(rep_array[0]))\n",
    "print(\"RV Dice avg: \",np.average(rep_array[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8553f7e9-0925-43b2-b8df-1abbe5df3b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MYO Dice min:  0.45842958288729885\n",
      "MYO Dice max:  0.8644414786006317\n",
      "MYO Dice avg:  0.7405527295192103\n"
     ]
    }
   ],
   "source": [
    "print(\"MYO Dice min: \",min(rep_array[1]))\n",
    "print(\"MYO Dice max: \",max(rep_array[1]))\n",
    "print(\"MYO Dice avg: \",np.average(rep_array[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8c869260-ea3b-464d-86f8-6e5333163677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LV Dice min:  0.522568112440621\n",
      "LV Dice max:  0.9594361408960964\n",
      "LV Dice avg:  0.9101154498015966\n"
     ]
    }
   ],
   "source": [
    "print(\"LV Dice min: \",min(rep_array[2]))\n",
    "print(\"LV Dice max: \",max(rep_array[2]))\n",
    "print(\"LV Dice avg: \",np.average(rep_array[2]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
